val lines = sc.textFile("hdfs://localhost:8020/tmp/benchmark/text/tiny/rankings")
#println(lines.first())
val rankingTable = lines.map(line => line.split(","))
#println(rankingTable.first())
val result = rankingTable.filter(row => row(1).toInt > 50)
result.foreach(line => println(line.deep.mkString(" ")))
#rankingTable.take(10).foreach(line => println(line.deep.mkString(" ")))


######################join_query##########################

val sc = new SparkContext(conf)
val ranking = sc.textFile("hdfs://localhost:8020/tmp/benchmark/text/tiny/rankings")
val uservisit = sc.textFile("hdfs://localhost:8020/tmp/benchmark/text/tiny/uservisits")
val rankingTable = ranking.map(line => line.split(","))
val uservisitTable = uservisit.map(line => line.split(","))
val format = new java.text.SimpleDateFormat("yyyy-MM-dd")
val start_date = format.parse("1980-01-01")
val end_date = format.parse("1980-04-01")
val validDate = uservisitTable.filter(line => format.parse(line(2)).compareTo(start_date) >= 0 && format.parse(line(2)).compareTo(end_date) <= 0)
val rankingPairs = rankingTable.map(line => (line(0), line))
val uservisitPairs = uservisitTable.map(line => (line(1), line))
val joinTable = uservisitPairs.join(rankingPairs)

#x._2._1(0)   #sourceIP
#x._2._2(1)   #pageRank
#x._2._1(3)   #adRevenue

val sourceIPAsKey = joinTable.map(x => (x._2._1(0), (x._2._2(1).toDouble, x._2._1(3).toDouble)))
val groupSourceIP = sourceIPAsKey.mapValues(x => (x, 1)).reduceByKey((x, y) => ((x._1._1 + y._1._1 , x._1._2 + y._1._2), x._2 + y._2))
val groupResult = groupSourceIP.map(x => (x._2._1._2, (x._2._1._1/x._2._2, x._1)))
val result = groupResult.sortByKey(false)
println(result.take(1)(0)._2._2, result.take(1)(0)._1, result.take(1)(0)._2._1)
